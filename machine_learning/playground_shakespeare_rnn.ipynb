{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "playground_shakespeare_rnn.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rYPYBeAhDWkj",
        "colab_type": "code",
        "outputId": "a9c0fec1-3429-46d7-e3b6-4e5ca868d1ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "cell_type": "code",
      "source": [
        "# Setup of libraries, mounting the Google Drive etc.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir('gdrive/My Drive/Colab Notebooks')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c--9Gk0AJpYd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4841d7cc-436b-46a8-9af2-20a39b252d9d"
      },
      "cell_type": "code",
      "source": [
        "path = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nlhLkUtNom9l",
        "colab_type": "code",
        "outputId": "56749122-de0d-4ab8-8836-cfb7cb7acb32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "text = open(path).read()\n",
        "\n",
        "# The length of text is the number of characters in it\n",
        "print('Length of the text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of the text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fjOId_ttpETM",
        "colab_type": "code",
        "outputId": "1b6499f2-5212-40f8-dd1a-5c030f63b93a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bz_h1J0qpfdi",
        "colab_type": "code",
        "outputId": "d2c2687b-ff74-4fa1-e2ec-526cdd94a327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JFgFQIdcpsam",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {c: i for i, c in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-3d6Yq-iqHk9",
        "colab_type": "code",
        "outputId": "3f2e80f4-1862-40a8-e7f6-040952eba973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "print('{')\n",
        "for char, _ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CJRRf6FYrJHv",
        "colab_type": "code",
        "outputId": "d6590f61-93b6-4cb0-f6d8-37db9c46daa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Show how the first 13 characters from the text mapped to integers\n",
        "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VIU6jYmkz0mS",
        "colab_type": "code",
        "outputId": "f0ea10c6-4c39-46b8-e3a3-1cbe9a84abad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // seq_length\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for c in char_dataset.take(5):\n",
        "    print(idx2char[c.numpy()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZeCH-MIK0xA3",
        "colab_type": "code",
        "outputId": "7cda44ad-d351-4564-fa83-d688a519b77c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pz9iKBLq1wJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    \n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5KslXorK2KX1",
        "colab_type": "code",
        "outputId": "b6b8f67b-2f03-46e6-e05a-30214f17ee29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2hdTI8jfWb9d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "11fb86e6-1830-4631-d876-2ec47d5bd156"
      },
      "cell_type": "code",
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DVl0OL0W2rqH",
        "colab_type": "code",
        "outputId": "eb5519c1-af78-47d1-c440-0e24d915cee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Batch size \n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences, \n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Yv0DJcI13MgM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary of chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension \n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t0OpF35QXGxs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if tf.test.is_gpu_available():\n",
        "    rnn = tf.keras.layers.CuDNNGRU\n",
        "else:\n",
        "    import functools\n",
        "    rnn = functools.partial(\n",
        "        tf.keras.layers.GRU, recurrent_activation='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8pC0ekUnjqH0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size,\n",
        "                                  embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        rnn(rnn_units,\n",
        "            return_sequences=True, \n",
        "            recurrent_initializer='glorot_uniform',\n",
        "            stateful=True),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ziLNtpcjs7w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size = len(vocab), \n",
        "                    embedding_dim=embedding_dim, \n",
        "                    rnn_units=rnn_units, \n",
        "                    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KKjCgFYw4BDe",
        "colab_type": "code",
        "outputId": "508ceb89-0631-4c56-9477-e56d802755d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1): \n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i_39lxgP4Kks",
        "colab_type": "code",
        "outputId": "f7840ddc-8220-4f0d-bf7c-d5711111b658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_2 (CuDNNGRU)       (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hj5DmeNK4QzR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.multinomial(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fPLalixCkJhG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5d2db9c2-1df4-4a38-9ff1-1c6c382b8ae8"
      },
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15, 60, 54, 38,  6, 63,  9, 38, 11, 40, 30, 42, 32,  6, 53, 55, 51,\n",
              "       43, 51, 46, 51, 30,  9, 29, 24, 56, 56, 22, 17, 56, 42, 35,  2, 14,\n",
              "       32, 60, 54, 36, 16, 42, 39, 55, 20, 55, 19, 33, 25,  8, 41, 12,  4,\n",
              "       45, 44, 22, 16, 39, 49, 25, 19, 31, 56, 31, 48, 13, 40, 28, 61, 16,\n",
              "       12, 44,  3, 28, 59, 41, 33, 14, 64, 55,  0, 54, 41, 53, 57, 37, 23,\n",
              "       42,  0, 56, 52, 27, 34, 57, 57,  2, 50, 53, 22, 58, 52, 46])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "-BuilRvf6F4F",
        "colab_type": "code",
        "outputId": "fc451d39-952b-48ff-c5fc-3e00d4a1f2ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \"n whilst\\n'Twixt you there's difference; but the fall of either\\nMakes the survivor heir of all.\\n\\nAUFI\"\n",
            "\n",
            "Next Char Predictions: \n",
            " 'CvpZ,y3Z;bRdT,oqmemhmR3QLrrJErdW!BTvpXDdaqHqGUM.c?&gfJDakMGSrSjAbPwD?f$PucUBzq\\npcosYKd\\nrnOVss!loJtnh'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cc5zK8jy6Xk9",
        "colab_type": "code",
        "outputId": "df98e096-aeea-4ed1-afdb-9ddf1bb57be0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.backend.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1756663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VFm9p3mE7E08",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
        "              loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85mMnbmA7NIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
        "                                                         save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qEvj6jUP7b-v",
        "colab_type": "code",
        "outputId": "459e0220-2d47-4570-cc26-32987fbb1217",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset.repeat(),\n",
        "                    epochs=EPOCHS,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    callbacks=[checkpoint_callback])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "174/174 [==============================] - 28s 158ms/step - loss: 2.7018\n",
            "Epoch 2/20\n",
            "174/174 [==============================] - 26s 151ms/step - loss: 1.9580\n",
            "Epoch 3/20\n",
            "174/174 [==============================] - 26s 151ms/step - loss: 1.6904\n",
            "Epoch 4/20\n",
            "174/174 [==============================] - 26s 152ms/step - loss: 1.5425\n",
            "Epoch 5/20\n",
            "174/174 [==============================] - 26s 151ms/step - loss: 1.4527\n",
            "Epoch 6/20\n",
            "174/174 [==============================] - 26s 152ms/step - loss: 1.3935\n",
            "Epoch 7/20\n",
            "174/174 [==============================] - 26s 151ms/step - loss: 1.3498\n",
            "Epoch 8/20\n",
            "174/174 [==============================] - 26s 152ms/step - loss: 1.3106\n",
            "Epoch 9/20\n",
            "174/174 [==============================] - 26s 152ms/step - loss: 1.2763\n",
            "Epoch 10/20\n",
            "174/174 [==============================] - 26s 151ms/step - loss: 1.2442\n",
            "Epoch 11/20\n",
            "174/174 [==============================] - 26s 151ms/step - loss: 1.2117\n",
            "Epoch 12/20\n",
            "174/174 [==============================] - 26s 151ms/step - loss: 1.1785\n",
            "Epoch 13/20\n",
            "174/174 [==============================] - 26s 151ms/step - loss: 1.1476\n",
            "Epoch 14/20\n",
            "174/174 [==============================] - 27s 155ms/step - loss: 1.1130\n",
            "Epoch 15/20\n",
            "174/174 [==============================] - 27s 153ms/step - loss: 1.0790\n",
            "Epoch 16/20\n",
            "174/174 [==============================] - 27s 153ms/step - loss: 1.0435\n",
            "Epoch 17/20\n",
            "174/174 [==============================] - 27s 153ms/step - loss: 1.0086\n",
            "Epoch 18/20\n",
            "174/174 [==============================] - 27s 153ms/step - loss: 0.9732\n",
            "Epoch 19/20\n",
            "174/174 [==============================] - 27s 153ms/step - loss: 0.9375\n",
            "Epoch 20/20\n",
            "174/174 [==============================] - 27s 153ms/step - loss: 0.9035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bXZzJ2JM-GLC",
        "colab_type": "code",
        "outputId": "e0c8a910-344d-4fb7-fb32-b376f8c86f67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_20'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "Dmt6Bqfi-HpV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MulFDoH_k8Q5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "80d0ea60-fb95-4841-b801-40bae82fac70"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_3 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vrQ44Ycq-UIt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "    \n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "    \n",
        "    # You can change the start string to experiment\n",
        "    start_string = 'ROMEO'\n",
        "    \n",
        "    # Converting our start string to numbers (vectorizing) \n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    \n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "    \n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "    \n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # using a multinomial distribution to predict the word returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        # We pass the predicted word as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        \n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dCJapROylibJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "41f41c84-45aa-40e3-d9cf-38a2fd2c1fac"
      },
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=\"ROMEO: \"))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Not by the service, and eberishouble things it,\n",
            "Or howl'd it farther.\n",
            "\n",
            "Of that heaven soon?\n",
            "If I may cross thee hunger, to Angelo;--herein quickly sent\n",
            "but wewh d you forth he would\n",
            "leave, you shall be in scorn wroughts,\n",
            "But tell me what thy wife is on an admon'd now but it\n",
            "be quick. Thou shalt not--\n",
            "But to her hearts, 'Rivia, like a poorly and as\n",
            "you beer my person: here's together shall\n",
            "The Lord of Wiltshire-pinch'd lookiss of this unhappy dropping brother,\n",
            "I will obey you not.\n",
            "\n",
            "GONZALO:\n",
            "When, Almisses, lords! I say so;\n",
            "For such earth ready to him, within this mine\n",
            "A ragged speece of Marcius.\n",
            "\n",
            "ISABELLA:\n",
            "Worse than hollow world, but that\n",
            "Ran after house, think now LicET:\n",
            "Here on my womb, and very welcome. But Willoud,\n",
            "God have I need none equall'd; and unreen\n",
            "Your time is spoils, on my heart sound fath him:\n",
            "Shortlus it most end, for thy revenge\n",
            "In this sworn of my greatness with their\n",
            "shotted, as 'twere as runawoursh, and afterwards\n",
            "Whet we can learn to course o' the commons' ears,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}